# Ответы на вопросы — Практическая работа №4

## 1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?

В CUDA существует несколько типов памяти, которые различаются по скорости доступа, объёму и области видимости:

- **Глобальная память (Global memory):**  
  Доступна всем потокам на GPU, используется для больших массивов данных. Доступ к ней **медленный**, особенно при случайных обращениях. Используется для хранения исходных данных и результатов, которые должны быть доступны всем потокам.

- **Разделяемая память (Shared memory):**  
  Общая для всех потоков одного блока. Очень быстрая, позволяет обмениваться данными между потоками блока. Используется для **промежуточных вычислений**, когда нужно уменьшить количество обращений к глобальной памяти.

- **Локальная память (Local memory / регистры):**  
  Хранится в регистрах потока или в локальной памяти GPU. Доступ очень быстрый, но объём ограничен. Используется для временных переменных внутри потока и ускорения вычислений, не требующих обмена данными между потоками.

---

## 2. Как использование разделяемой памяти влияет на производительность?

- Разделяемая память **значительно ускоряет вычисления**, так как задержка доступа к ней в десятки раз меньше, чем к глобальной памяти.  
- Она позволяет хранить промежуточные результаты и обмениваться ими между потоками блока без обращения к медленной глобальной памяти.  
- Пример: в редукции суммы элементов массива использование shared memory вместо глобальной уменьшает время выполнения почти в 5–10 раз.

---

## 3. Доступ и как его обеспечить?

- **Глобальная память:** доступ через указатели на массивы, переданные в ядро. Доступ ко всем элементам возможен из любого потока.  
- **Разделяемая память:** объявляется внутри ядра с ключевым словом `__shared__`. Потоки одного блока могут читать и писать данные, но нужно использовать `__syncthreads()` для синхронизации, чтобы все потоки дождались записи перед чтением.  
- **Локальная память:** обычные переменные внутри ядра (`int`, `float`), выделяются автоматически на поток.

---

## 4. Какие сложности возникают при работе с большим объемом данных на GPU?

- Ограниченный объём **глобальной памяти** и **разделяемой памяти на блок**, что требует деления данных на блоки.  
- **Бутылочные горлышки при доступе к глобальной памяти**: если много потоков одновременно обращаются к разным ячейкам, производительность падает.  
- Необходимость правильно синхронизировать потоки для shared memory.  
- При больших массивах сортировка или редукция без оптимизаций может быть медленной.

---

## 5. Почему важно минимизировать доступ к глобальной памяти?

- Доступ к глобальной памяти **очень медленный** (десятки или сотни тактов).  
- Чем меньше обращений к глобальной памяти, тем выше общая производительность.  
- Использование **разделяемой памяти** для хранения промежуточных результатов позволяет сократить число обращений к глобальной памяти и ускорить алгоритм.

---

## 6. Как использовать профилирование для анализа производительности CUDA-программ?

- CUDA Toolkit предоставляет инструменты: **Nsight Compute**, **Nsight Systems**, **Visual Profiler**.  
- С их помощью можно:  
  - Измерять время выполнения ядер (kernels)  
  - Анализировать использование памяти (глобальной, shared, локальной)  
  - Определять узкие места (bottlenecks) в коде  
  - Сравнивать эффективность разных вариантов реализации алгоритма  
- Профилирование помогает понять, где нужно использовать shared memory, оптимизировать доступ к памяти и добиться ускорения.

---
